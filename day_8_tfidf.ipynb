{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 建立TFIDF模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "參考[https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$tf-idf = tf * idf$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\n",
    "documentA = 'the man went out for a walk'\n",
    "documentB = 'the children sat around the fire'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 做tokenize，取出所有文件的單字"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenize_A = nltk.word_tokenize(documentA)\n",
    "tokenize_B = nltk.word_tokenize(documentB)\n",
    "\n",
    "uniqueWords = set(tokenize_A).union(set(tokenize_B)) # 所有文件的單詞"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 計算每個文件，所有uniqueWords出現的次數"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "numofwordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in tokenize_A:\n",
    "    numofwordsA[word] += 1\n",
    "numofwordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in tokenize_B:\n",
    "    numofwordsB[word] += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "numofwordsA"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'fire': 0,\n",
       " 'sat': 0,\n",
       " 'children': 0,\n",
       " 'walk': 1,\n",
       " 'for': 1,\n",
       " 'around': 0,\n",
       " 'went': 1,\n",
       " 'a': 1,\n",
       " 'man': 1,\n",
       " 'out': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "numofwordsB"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'the': 2,\n",
       " 'fire': 1,\n",
       " 'sat': 1,\n",
       " 'children': 1,\n",
       " 'walk': 0,\n",
       " 'for': 0,\n",
       " 'around': 1,\n",
       " 'went': 0,\n",
       " 'a': 0,\n",
       " 'man': 0,\n",
       " 'out': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 計算TF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$tf_{i,j} = \\frac{n_{i,j}}{\\sum_{k}(n_{i, j})}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF: The number of times a word appears in a document divded by the total number of words in the document."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def computeTF(wordDict, tokenize_item):\n",
    "    '''\n",
    "    wordDict : 文件內單詞所出現的字典\n",
    "    tokenize_item : 文件tokenize後的輸出\n",
    "    '''\n",
    "    tfDict = {}\n",
    "    bagofwordscount = len(tokenize_item) # tokenize_item 單字數量\n",
    "    for word , count in wordDict.items():\n",
    "        tfDict[word] = count / bagofwordscount #單詞在該文件出現次數 / 該文件擁有所有單詞數量\n",
    "    return tfDict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 計算IDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$idf = log(\\frac{N}{df_t})$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "IDF = Inverse data frequency determines the weight of rare words across all documents in the corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def computeIDF(documentsDict):\n",
    "    '''\n",
    "    documentsDict: 為一個list，包含所有文件的wordDict\n",
    "    '''\n",
    "    import math\n",
    "    N = len(documentsDict)\n",
    "\n",
    "    idfDict = dict.fromkeys(documentsDict[0].keys(), 0)\n",
    "    for document in documentsDict:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1 ##計算單詞在多少文件中出現過\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / val) #計算IDF, log(所有文件的數目/包含這個單詞的文件數目)\n",
    "    \n",
    "    return idfDict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 計算TFIDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$w_{i,j} = tf_{i,j}*log(\\dfrac{N}{df_i})$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def computeTFIDF(tf_item, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tf_item.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tfA = computeTF(numofwordsA, tokenize_A)\n",
    "tfB = computeTF(numofwordsB, tokenize_B)\n",
    "\n",
    "idfs = computeIDF([numofwordsA, numofwordsB])\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tfidfA"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'the': 0.0,\n",
       " 'fire': 0.0,\n",
       " 'sat': 0.0,\n",
       " 'children': 0.0,\n",
       " 'walk': 0.09902102579427789,\n",
       " 'for': 0.09902102579427789,\n",
       " 'around': 0.0,\n",
       " 'went': 0.09902102579427789,\n",
       " 'a': 0.09902102579427789,\n",
       " 'man': 0.09902102579427789,\n",
       " 'out': 0.09902102579427789}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tfidfB"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'the': 0.0,\n",
       " 'fire': 0.11552453009332421,\n",
       " 'sat': 0.11552453009332421,\n",
       " 'children': 0.11552453009332421,\n",
       " 'walk': 0.0,\n",
       " 'for': 0.0,\n",
       " 'around': 0.11552453009332421,\n",
       " 'went': 0.0,\n",
       " 'a': 0.0,\n",
       " 'man': 0.0,\n",
       " 'out': 0.0}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}